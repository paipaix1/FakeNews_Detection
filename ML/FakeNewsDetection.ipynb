{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "import re  \n",
    "import jieba  \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn import metrics \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国反腐风刮到阿根廷，这个美到让人瘫痪的女总统，因为8个本子摊上大事了\n",
      "0\n",
      "总的数据量: 10000\n"
     ]
    }
   ],
   "source": [
    "def get_data():  \n",
    "\n",
    "    df = pd.read_csv('./Data/train.news.csv')\n",
    "    df_sample = df.sample(n=10000, random_state=42) #随机抽取10000个样本\n",
    "    corpus = df_sample['Title']\n",
    "    labels = df_sample['label']\n",
    "    return corpus, labels  \n",
    "\n",
    "\n",
    "corpus, labels = get_data()     # 获取数据集\n",
    "print(corpus[0])                # 示例输出\n",
    "print(labels[0])                \n",
    "print(\"总的数据量:\", len(labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冯小刚还不知道自己是怎么死的！\n",
      "1\n",
      "总的数据量: 5000\n"
     ]
    }
   ],
   "source": [
    "def remove_empty_docs(corpus, labels):  \n",
    "    '''\n",
    "    功能：重新整合邮件及标签，去除空白邮件\n",
    "    zip(corpus, labels),控制同时遍历corpus和labels\n",
    "    doc.strip(),去除文本首尾空格和换行符，如果有非空字符，返回真\n",
    "    '''\n",
    "    filtered_corpus = []  \n",
    "    filtered_labels = []  \n",
    "    for doc, label in zip(corpus, labels):  \n",
    "        if doc.strip():  \n",
    "            filtered_corpus.append(doc)  \n",
    "            filtered_labels.append(label)  \n",
    "  \n",
    "    return filtered_corpus, filtered_labels \n",
    "\n",
    "corpus, labels = remove_empty_docs(corpus, labels) \n",
    "label_name_map = [\"垃圾邮件\", \"正常邮件\"]  \n",
    "print(corpus[0])      # 输出处理后的第一封邮件(换行符被处理在字符串中)\n",
    "print(labels[0])      \n",
    "print(\"总的数据量:\", len(labels))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量：3500，测试样本数量：1500\n",
      "公安已提醒！微信上所有的朋友请看下..... 0\n",
      "作为刑警，也来说说滴滴 0\n"
     ]
    }
   ],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):  \n",
    "    '''\n",
    "    功能：划分数据集\n",
    "    test_size=0.3，测试集30%，训练集70%\n",
    "    random_state=42，设置随机种子，确保每次切割结果一致\n",
    "    '''  \n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size = test_data_proportion, random_state=42)  \n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,  labels,  test_data_proportion=0.3)  \n",
    "print('训练集样本数量：%d，测试样本数量：%d'%(len(train_corpus),len(test_corpus)))\n",
    "print(train_corpus[101],train_labels[101])\n",
    "print(test_corpus[3],test_labels[3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "罗志 祥和 周扬青 同居 试婚 周扬青 整容\n",
      "人贩子 没有 孩子 新型 拐卖 防不胜防\n"
     ]
    }
   ],
   "source": [
    "def normalize_corpus(corpus):  \n",
    "    '''\n",
    "    功能：导入停用词于stopwords列表,遍历corpus（即每一行）并分词，将每一行中不是停用词的词连成字符串（词与词用空格隔开），再存入normalized_corpus列表\n",
    "    sw.replace('\\n', '')，将sw通过readlines到的每一行内容中的换行符移除\n",
    "    jieba.lcut(text.replace('\\n','')),对字符串去除空格和换行符后，做分词处理放入列表中\n",
    "    text = ' '.join(filtered_tokens)，将filterd_tokens列表元素拼接为字符串(用空格隔开元素)\n",
    "    '''\n",
    "    normalized_corpus = []  \n",
    "    stopwords = [sw.replace('\\n', '') for sw in open('./Data/stopwords.txt',encoding='utf-8').readlines()]   \n",
    "        \n",
    "    for text in corpus:  \n",
    "        filtered_tokens = []   \n",
    "        tokens = jieba.lcut(text.replace('\\n',''))  \n",
    "  \n",
    "        for token in tokens:  \n",
    "            token = token.strip()  \n",
    "            if token not in stopwords and len(token)>1:  \n",
    "                filtered_tokens.append(token)  \n",
    "          \n",
    "        text = ' '.join(filtered_tokens)  \n",
    "        normalized_corpus.append(text)  \n",
    "    return normalized_corpus\n",
    "\n",
    "norm_train_corpus = normalize_corpus(train_corpus)  \n",
    "norm_test_corpus = normalize_corpus(test_corpus) \n",
    "print(norm_train_corpus[0])\n",
    "print(norm_test_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 8690)\n",
      "(1500, 8690)\n",
      "(3500, 8690)\n",
      "(1500, 8690)\n"
     ]
    }
   ],
   "source": [
    "def bow_extractor(normalized_corpus, ngram_range=(1, 1)):  \n",
    "    '''\n",
    "    功能：将normalized_corpus（分词处理后的）转化为BOW词袋模型\n",
    "    min_df=1，表示包含至少在一个文档中（即每一行）出现的词汇才会被纳入词典\n",
    "    ngram_range=(1, 1)，定义n-gram的范围，只提取单个词汇，不考虑组合词汇\n",
    "    fit_transform(normalized_corpus)，文本数据转化为词频矩阵，每一行代表一个文档，每一列代表一个词汇项，矩阵中的元素表示相应词汇在该文档(这一行中)的出现次数\n",
    "    features.shape = (len(normalized_corpus),dic_size)，dic_size为词典大小\n",
    "    ''' \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)  \n",
    "    features = vectorizer.fit_transform(normalized_corpus)  \n",
    "    return vectorizer, features\n",
    "    \n",
    "def tfidf_extractor(normalized_corpus, ngram_range=(1, 1)):  \n",
    "    '''\n",
    "    功能：将normalized_corpus（分词处理后的）转化为TF-IDF表示形式\n",
    "    norm = 'l2',在计算TF-IDF向量后，会对结果向量做L2范数归一化\n",
    "    fit_transform(normalized_corpus),将文本数据转换为TF-IDF矩阵，每一行代表一个文档，每一列代表一个词汇项,矩阵中的元素表示相应词汇在文档中的TF-IDF值\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(min_df=1,  \n",
    "                                 norm='l2',  \n",
    "                                 smooth_idf=True,       # 在IDF权重计算时引入平滑项，防止出现零概率问题\n",
    "                                 use_idf=True,          # 启用IDF调整，即TF-IDF计算公式中的逆文档频率部分\n",
    "                                 ngram_range=ngram_range)  \n",
    "    features = vectorizer.fit_transform(normalized_corpus)  \n",
    "    return vectorizer, features\n",
    "\n",
    "# BOW词袋模型\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)  # 将训练分词集传入函数中，获取训练集vector和features\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)         # 将测试分词集传入训练集vector中，获得测试集features\n",
    "print(bow_train_features.shape)                                        # (7000,2473)表示训练集7000封邮件，词典size为27473\n",
    "print(bow_test_features.shape)\n",
    "# print(bow_train_features)\n",
    "# print(bow_test_features)\n",
    "\n",
    "# TF-IDF模型(同理)\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "print(tfidf_train_features.shape)\n",
    "print(tfidf_test_features.shape)\n",
    "# print(tfidf_train_features)\n",
    "# print(tfidf_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于词袋模型特征的逻辑回归\n",
      "准确率:0.9333\n",
      "精度:0.9340\n",
      "召回率:0.9333\n",
      "F1得分:0.9313\n",
      "基于词袋模型的支持向量机\n",
      "准确率:0.9327\n",
      "精度:0.9319\n",
      "召回率:0.9327\n",
      "F1得分:0.9317\n",
      "基于tfidf的逻辑回归模型\n",
      "准确率:0.9020\n",
      "精度:0.9082\n",
      "召回率:0.9020\n",
      "F1得分:0.8952\n",
      "基于tfidf的支持向量机模型\n",
      "准确率:0.9420\n",
      "精度:0.9414\n",
      "召回率:0.9420\n",
      "F1得分:0.9415\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(true_labels, predicted_labels): \n",
    "    '''\n",
    "    功能：计算预测的各类指标\n",
    "    ''' \n",
    "    acc = metrics.accuracy_score(true_labels,predicted_labels)  \n",
    "    precision = metrics.precision_score(true_labels, predicted_labels, average = 'weighted')  \n",
    "    recall = metrics.recall_score(true_labels, predicted_labels,average='weighted')  \n",
    "    f1_score = metrics.f1_score(true_labels,predicted_labels,average='weighted')  \n",
    "    print('准确率:%.4f' % acc)  \n",
    "    print('精度:%.4f' % precision)  \n",
    "    print('召回率:%.4f' % recall)  \n",
    "    print('F1得分:%.4f' % f1_score)  \n",
    "  \n",
    "def train_predict(classifier, train_features, train_labels, test_features, test_labels):  \n",
    "    '''\n",
    "    功能：根据传入的训练分类器，进行模型训练\n",
    "    '''\n",
    "    classifier.fit(train_features, train_labels)  # 喂给模型训练矩阵和训练标签\n",
    "    predictions = classifier.predict(test_features)  # 模型预测\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)  # 调用get_metrics函数,打印预测结果的各类指标\n",
    "    return predictions\n",
    "\n",
    "# 分类器  \n",
    "svm = SGDClassifier(loss='hinge')  \n",
    "lr = LogisticRegression()  \n",
    "# 基于词袋模型特征的逻辑回归  \n",
    "print(\"基于词袋模型特征的逻辑回归\")  \n",
    "lr_bow_predictions = train_predict(classifier=lr,  \n",
    "                            train_features=bow_train_features,  \n",
    "                            train_labels=train_labels,  \n",
    "                            test_features=bow_test_features,  \n",
    "                            test_labels=test_labels)  \n",
    "\n",
    "# 基于词袋模型的支持向量机方法  \n",
    "print(\"基于词袋模型的支持向量机\")  \n",
    "svm_bow_predictions = train_predict(classifier=svm,  \n",
    "                            train_features=bow_train_features,  \n",
    "                            train_labels=train_labels,  \n",
    "                            test_features=bow_test_features,  \n",
    "                            test_labels=test_labels)  \n",
    "\n",
    "# 基于tfidf的逻辑回归模型  \n",
    "print(\"基于tfidf的逻辑回归模型\")  \n",
    "lr_tfidf_predictions = train_predict(classifier=lr,  \n",
    "                            train_features=tfidf_train_features,  \n",
    "                            train_labels=train_labels,  \n",
    "                            test_features=tfidf_test_features,  \n",
    "                            test_labels=test_labels)  \n",
    "\n",
    "# 基于tfidf的支持向量机模型  \n",
    "print(\"基于tfidf的支持向量机模型\")  \n",
    "svm_tfidf_predictions = train_predict(classifier=svm,  \n",
    "                            train_features=tfidf_train_features,  \n",
    "                            train_labels=train_labels,  \n",
    "                            test_features=tfidf_test_features,  \n",
    "                            test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM预测结果： [0]\n"
     ]
    }
   ],
   "source": [
    "# 以基于词袋模型的SVM为例\n",
    "# 创建SVM分类器\n",
    "svm_classifier = SGDClassifier(loss='hinge')  \n",
    "# 模型训练\n",
    "svm_classifier.fit(bow_train_features,train_labels)\n",
    "# 模型保存\n",
    "joblib.dump(svm_classifier, 'svm_model.pkl')\n",
    "# 模型加载\n",
    "loaded_svm_model = joblib.load('svm_model.pkl')\n",
    "\n",
    "# 输入预测内容\n",
    "new_data = ['老兵日记,2018公安改革:明年3月全国义务兵转正!成为人民警察编']\n",
    "# 标准化\n",
    "train_data = normalize_corpus(new_data)\n",
    "# 特征提取\n",
    "test_features = bow_vectorizer.transform(train_data)\n",
    "# 模型预测\n",
    "svm_prediction = loaded_svm_model.predict(test_features)\n",
    "print(\"SVM预测结果：\", svm_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3525df8de6fccdb833c6f55bb2d91ae8d8eb60979ee404447b318d93de5ec75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
